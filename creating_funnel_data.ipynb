{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Utils file to fetch the Raw data which are in that case Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing the shape for dataframe and creating index column that will be used in lookup channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    def concatenate_columns(row):\n",
    "        try:\n",
    "            return row['sessionSourceMedium'] + row['sessionCampaignName']\n",
    "        except Exception as e:\n",
    "            print(f\"Error in concatenating columns: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Convert the 'date' column to a datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d').dt.strftime('%m/%d/%Y')\n",
    "    df['sessionCampaignName'] = df['sessionCampaignName'].apply(lambda x: x[1:] if isinstance(x, str) and x.startswith('=') else x)\n",
    "\n",
    "    # Create a new column 'Index' by concatenating 'sessionSourceMedium' and 'sessionCampaignName'\n",
    "    df['Index'] = df.apply(concatenate_columns, axis=1)\n",
    "\n",
    "    # Create 'source/medium_index_date' column by concatenating 'sessionSourceMedium', 'sessionCampaignName', and 'date'\n",
    "    df['source/medium_index_date'] = pd.concat([df['sessionSourceMedium'], df['sessionCampaignName'], df['date']], axis=1).apply(''.join, axis=1)\n",
    "\n",
    "    # Set 'date' as the index\n",
    "    df.set_index('date', append=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlining Data Processing and File Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, output_directory, processed_directory):\n",
    "        self.output_directory = output_directory\n",
    "        self.processed_directory = processed_directory\n",
    "        self.date_pattern = r\"\\d{4}-\\d{2}-\\d{2}\"\n",
    "\n",
    "        # Ensure the processed data directory exists\n",
    "        if not os.path.exists(self.processed_directory):\n",
    "            os.makedirs(self.processed_directory)\n",
    "            print(f'Created directory: {self.processed_directory}')\n",
    "        else:\n",
    "            os.makedirs(self.processed_directory, exist_ok=True)\n",
    "            print('Processed folder already exists', self.processed_directory)\n",
    "            print('===============================================')\n",
    "\n",
    "    def process_data(self):\n",
    "        if os.path.exists(self.output_directory):\n",
    "            output_files = os.listdir(self.output_directory)\n",
    "\n",
    "            for output_file in output_files:\n",
    "                match = re.search(self.date_pattern, output_file)\n",
    "\n",
    "                if match:\n",
    "                    date_string = match.group()\n",
    "                    processed_file_name = f'Processed_Sessions_{date_string}.csv'\n",
    "                    output_file_path = os.path.join(self.output_directory, output_file)\n",
    "                    processed_file_path = os.path.join(self.processed_directory, processed_file_name)\n",
    "\n",
    "                    if os.path.exists(processed_file_path):\n",
    "                        choice = input(f'The file {processed_file_name} already exists. Do you want to (O)verwrite or (C)ontinue? [O/C]: ').strip().lower()\n",
    "                        if choice == 'o':\n",
    "                            print(f'Overwriting the existing file {processed_file_name}')\n",
    "                            self.process_file(output_file_path, processed_file_path)\n",
    "                        elif choice == 'c':\n",
    "                            print('Continuing with the existing file.')\n",
    "                            self.continue_with_existing_file(processed_file_path)\n",
    "                        else:\n",
    "                            print('Invalid choice. Please enter \"O\" to overwrite or \"C\" to continue.')\n",
    "                    else:\n",
    "                        self.process_file(output_file_path, processed_file_path)\n",
    "                else:\n",
    "                    print(f\"No date pattern found in filename: {output_file}\")\n",
    "        else:\n",
    "            print(\"Output data directory does not exist.\")\n",
    "\n",
    "    def process_file(self, input_file_path, output_file_path):\n",
    "        # Add your code to process the file and save it in the processed directory\n",
    "        raw_df = pd.read_csv(input_file_path)\n",
    "        # print(raw_df.columns)\n",
    "        raw_df = process_dataframe(raw_df)\n",
    "        raw_df.reset_index(inplace=True)\n",
    "        print(raw_df.columns)\n",
    "\n",
    "        event_types = ['session_start', 'add_to_cart', 'remove_from_cart', 'begin_checkout', 'purchase']\n",
    "        # Create dictionaries to store aggregated data\n",
    "        agg_data = {event: raw_df[raw_df['eventName'] == event].groupby('Index')['sessions'].sum() for event in event_types}\n",
    "\n",
    "        # Create a DataFrame using the 'Index' as the index\n",
    "        merged_df = pd.DataFrame(index=raw_df['Index'])\n",
    "\n",
    "        # Populate the merged_df with aggregated data from the dictionaries\n",
    "        for event in event_types:\n",
    "            merged_df[event] = agg_data[event]\n",
    "\n",
    "        # Fill missing values with 0\n",
    "        merged_df.fillna(0, inplace=True)\n",
    "        merged_df.reset_index(inplace=True)\n",
    "        merged_df.drop_duplicates(subset='Index', inplace=True)\n",
    "\n",
    "        final_merge = merged_df.merge(raw_df[['date', 'sessionSourceMedium', 'sessionCampaignName', 'Index']], on='Index', how='left')\n",
    "\n",
    "        final_merge.rename(columns={'session_start': 'Sessions', 'add_to_cart': 'ATC', 'remove_from_cart': 'Remove_From_Cart', 'begin_checkout': 'Checkout', 'purchase': 'Orders', 'sessionCampaignName': 'Campaign', 'sessionSourceMedium': 'Source / Medium'}, inplace=True)\n",
    "\n",
    "\n",
    "        final_merge.drop_duplicates(subset='Index', inplace=True)\n",
    "        desired_columns = ['date', 'Source / Medium', 'Campaign', 'Index', 'Sessions', 'ATC', 'Remove_From_Cart', 'Checkout', 'Orders']\n",
    "        final_merge = final_merge[desired_columns]\n",
    "        # Save the processed data to the output file\n",
    "        final_merge.to_csv(output_file_path, index=False)\n",
    "        print(f'Processed file Created At {output_file_path}')\n",
    "\n",
    "    def continue_with_existing_file(self, file_path):\n",
    "        print('continuing with existing file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the functions from where it query and to where it will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage\n",
    "output_directory = \"./raw_data\"\n",
    "processed_directory = \"./processed_funnel_data\"\n",
    "\n",
    "data_processor = DataProcessor(output_directory, processed_directory)\n",
    "data_processor.process_data()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
